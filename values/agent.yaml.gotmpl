vector:
  enable: true
  # enabled manually in ClusterVectorPipeline
  internalMetrics: false
  name: "vector"
  optimizeKubeSourceConfig: false
  agent:
    # based on recommendations
    # https://vector.dev/docs/setup/going-to-prod/sizing/#instance-types
    resources:
      limits:
        cpu: 8
        memory: 16Gi
      requests:
        cpu: 10m
        memory: 64Mi
    # collect logs also from ControlPlane nodes
    tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
        operator: Exists
      - effect: NoSchedule
        key: node.kubernetes.io/disk-pressure
        operator: Exists
      - operator: Exists
    priorityClassName: system-node-critical
    dataDir: /var/lib/vector
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - CHOWN
        - DAC_OVERRIDE
        - FOWNER
        - FSETID
        - KILL
        - NET_BIND_SERVICE
        - SETGID
        - SETPCAP
        - SETUID
      readOnlyRootFilesystem: true
      seLinuxOptions:
        type: spc_t
      seccompProfile:
        type: RuntimeDefault
    image: timberio/vector:0.31.0-debian
    volumes:
      - hostPath:
          path: /var/log/containers
          type: ""
        name: varlogcontainers
      - hostPath:
          path: /var/log/pods
          type: ""
        name: varlogpods
      - hostPath:
          path: /var/log/journal
          type: ""
        name: varlogjournal
      - hostPath:
          path: /var/log/audit
          type: ""
        name: varlogaudit
      - hostPath:
          path: /var/log/ovn
          type: ""
        name: varlogovn
      - hostPath:
          path: /var/log/oauth-apiserver
          type: ""
        name: varlogoauthapiserver
      - hostPath:
          path: /var/log/oauth-server
          type: ""
        name: varlogoauthserver
      - hostPath:
          path: /var/log/openshift-apiserver
          type: ""
        name: varlogopenshiftapiserver
      - hostPath:
          path: /var/log/kube-apiserver
          type: ""
        name: varlogkubeapiserver
      - emptyDir:
          medium: Memory
        name: tmp
    volumeMounts:
      - mountPath: /var/log/containers
        name: varlogcontainers
        readOnly: true
      - mountPath: /var/log/pods
        name: varlogpods
        readOnly: true
      - mountPath: /var/log/journal
        name: varlogjournal
        readOnly: true
      - mountPath: /var/log/audit
        name: varlogaudit
        readOnly: true
      - mountPath: /var/log/ovn
        name: varlogovn
        readOnly: true
      - mountPath: /var/log/oauth-apiserver
        name: varlogoauthapiserver
        readOnly: true
      - mountPath: /var/log/oauth-server
        name: varlogoauthserver
        readOnly: true
      - mountPath: /var/log/openshift-apiserver
        name: varlogopenshiftapiserver
        readOnly: true
      - mountPath: /var/log/kube-apiserver
        name: varlogkubeapiserver
        readOnly: true
      - mountPath: /tmp
        name: tmp

clustervectorpipeline:
  - name: "platform-logs"
    sources:
      raw_container_logs:
        type: kubernetes_logs
        glob_minimum_cooldown_ms: 15000
        auto_partial_merge: true
        use_apiserver_cache: true
        exclude_paths_glob_patterns:
          # do not collect logging logs, it can flood your sink
          - /var/log/pods/{{ .Release.Namespace }}*/.log
          - /var/log/pods/*/*/*.gz
          - /var/log/pods/*/*/*.tmp
        pod_annotation_fields:
          pod_labels: kubernetes.labels
          pod_namespace: kubernetes.namespace_name
          pod_annotations: kubernetes.annotations
          pod_uid: kubernetes.pod_id
          pod_node_name: hostname
      raw_journal_logs:
        type: journald
        journal_directory: /var/log/journal
      raw_host_audit_logs:
        type: file
        include:
          - /var/log/audit/audit.log
        host_key: hostname
        glob_minimum_cooldown_ms: 15000
      raw_k8s_audit_logs:
        type: file
        include:
          - /var/log/kube-apiserver/audit.log
        host_key: hostname
        glob_minimum_cooldown_ms: 15000
      raw_openshift_audit_logs:
        type: file
        include:
          - /var/log/oauth-apiserver/audit.log
          - /var/log/openshift-apiserver/audit.log
          - /var/log/oauth-server/audit.log
        host_key: hostname
        glob_minimum_cooldown_ms: 15000
      raw_ovn_audit_logs:
        type: file
        include:
          - /var/log/ovn/acl-audit-log.log
        host_key: hostname
        glob_minimum_cooldown_ms: 15000
      internal_metrics:
        type: internal_metrics
    transforms:
      container_logs:
        type: remap
        inputs:
          - raw_container_logs
        source: >2
            if !exists(.level) {
              .level = "default"
              if match!(.message, r'Info|INFO|^I[0-9]+|level=info|Value:info|"level":"info"|<info>') {
                .level = "info"
              } else if match!(.message, r'Warning|WARN|^W[0-9]+|level=warn|Value:warn|"level":"warn"|<warn>') {
                .level = "warn"
              } else if match!(.message, r'Error|ERROR|^E[0-9]+|level=error|Value:error|"level":"error"|<error>') {
                .level = "error"
              } else if match!(.message, r'Critical|CRITICAL|^C[0-9]+|level=critical|Value:critical|"level":"critical"|<critical>') {
                .level = "critical"
              } else if match!(.message, r'Debug|DEBUG|^D[0-9]+|level=debug|Value:debug|"level":"debug"|<debug>') {
                .level = "debug"
              } else if match!(.message, r'Notice|NOTICE|^N[0-9]+|level=notice|Value:notice|"level":"notice"|<notice>') {
                .level = "notice"
              } else if match!(.message, r'Alert|ALERT|^A[0-9]+|level=alert|Value:alert|"level":"alert"|<alert>') {
                .level = "alert"
              } else if match!(.message, r'Emergency|EMERGENCY|^EM[0-9]+|level=emergency|Value:emergency|"level":"emergency"|<emergency>') {
                .level = "emergency"
              }
            }
            del(.source_type)
            del(.stream)
            del(.kubernetes.pod_ips)

            parsed, err = parse_json(.message)
            if err == null {
              .structured = parsed
            }

            # Dedot
            . = map_keys(., recursive: true) -> |key| { replace(key, ".", "_") }

            if exists(.kubernetes.annotations.createdAt) { del(.kubernetes.annotations.createdAt) }
            if exists(.structured.ts) { del(.structured.ts) }
            if exists(.structured.took) { del(.structured.took) }
            if exists(.structured.ExternalSecret) { del(.structured.ExternalSecret) }

            ts = del(.timestamp); if !exists(."@timestamp") {."@timestamp" = ts}
      drop_priority_journal_logs:
        type: filter
        inputs:
          - raw_journal_logs
        condition: .PRIORITY != "7" && .PRIORITY != 7
      drop_audit_journal_logs:
        type: filter
        inputs:
          - drop_priority_journal_logs
        condition: .SYSLOG_IDENTIFIER != "audit"
      journal_logs:
        type: remap
        inputs:
          - drop_audit_journal_logs
        source: >2
            .tag = ".journal.system"

            del(.source_type)
            del(._CPU_USAGE_NSEC)
            del(.__REALTIME_TIMESTAMP)
            del(.__MONOTONIC_TIMESTAMP)
            del(._SOURCE_REALTIME_TIMESTAMP)
            del(.JOB_RESULT)
            del(.JOB_TYPE)
            del(.TIMESTAMP_BOOTTIME)
            del(.TIMESTAMP_MONOTONIC)

            if .PRIORITY == "8" || .PRIORITY == 8 {
              .level = "trace"
            } else {
              priority = to_int!(.PRIORITY)
              .level, err = to_syslog_level(priority)
              if err != null {
                log("Unable to determine level from PRIORITY: " + err, level: "error")
                log(., level: "error")
                .level = "unknown"
              } else {
                del(.PRIORITY)
              }
            }

            .hostname = del(.host)

            # systemd's kernel-specific metadata.
            # .systemd.k = {}
            if exists(.KERNEL_DEVICE) { .systemd.k.KERNEL_DEVICE = del(.KERNEL_DEVICE) }
            if exists(.KERNEL_SUBSYSTEM) { .systemd.k.KERNEL_SUBSYSTEM = del(.KERNEL_SUBSYSTEM) }
            if exists(.UDEV_DEVLINK) { .systemd.k.UDEV_DEVLINK = del(.UDEV_DEVLINK) }
            if exists(.UDEV_DEVNODE) { .systemd.k.UDEV_DEVNODE = del(.UDEV_DEVNODE) }
            if exists(.UDEV_SYSNAME) { .systemd.k.UDEV_SYSNAME = del(.UDEV_SYSNAME) }

            # trusted journal fields, fields that are implicitly added by the journal and cannot be altered by client code.
            .systemd.t = {}
            if exists(._AUDIT_LOGINUID) { .systemd.t.AUDIT_LOGINUID = del(._AUDIT_LOGINUID) }
            if exists(._BOOT_ID) { .systemd.t.BOOT_ID = del(._BOOT_ID) }
            if exists(._AUDIT_SESSION) { .systemd.t.AUDIT_SESSION = del(._AUDIT_SESSION) }
            if exists(._CAP_EFFECTIVE) { .systemd.t.CAP_EFFECTIVE = del(._CAP_EFFECTIVE) }
            if exists(._CMDLINE) { .systemd.t.CMDLINE = del(._CMDLINE) }
            if exists(._COMM) { .systemd.t.COMM = del(._COMM) }
            if exists(._EXE) { .systemd.t.EXE = del(._EXE) }
            if exists(._GID) { .systemd.t.GID = del(._GID) }
            if exists(._HOSTNAME) { .systemd.t.HOSTNAME = .hostname }
            if exists(._LINE_BREAK) { .systemd.t.LINE_BREAK = del(._LINE_BREAK) }
            if exists(._MACHINE_ID) { .systemd.t.MACHINE_ID = del(._MACHINE_ID) }
            if exists(._PID) { .systemd.t.PID = del(._PID) }
            if exists(._SELINUX_CONTEXT) { .systemd.t.SELINUX_CONTEXT = del(._SELINUX_CONTEXT) }
            if exists(._SOURCE_REALTIME_TIMESTAMP) { .systemd.t.SOURCE_REALTIME_TIMESTAMP = del(._SOURCE_REALTIME_TIMESTAMP) }
            if exists(._STREAM_ID) { .systemd.t.STREAM_ID = ._STREAM_ID }
            if exists(._SYSTEMD_CGROUP) { .systemd.t.SYSTEMD_CGROUP = del(._SYSTEMD_CGROUP) }
            if exists(._SYSTEMD_INVOCATION_ID) {.systemd.t.SYSTEMD_INVOCATION_ID = ._SYSTEMD_INVOCATION_ID}
            if exists(._SYSTEMD_OWNER_UID) { .systemd.t.SYSTEMD_OWNER_UID = del(._SYSTEMD_OWNER_UID) }
            if exists(._SYSTEMD_SESSION) { .systemd.t.SYSTEMD_SESSION = del(._SYSTEMD_SESSION) }
            if exists(._SYSTEMD_SLICE) { .systemd.t.SYSTEMD_SLICE = del(._SYSTEMD_SLICE) }
            if exists(._SYSTEMD_UNIT) { .systemd.t.SYSTEMD_UNIT = del(._SYSTEMD_UNIT) }
            if exists(._SYSTEMD_USER_UNIT) { .systemd.t.SYSTEMD_USER_UNIT = del(._SYSTEMD_USER_UNIT) }
            if exists(._TRANSPORT) { .systemd.t.TRANSPORT = del(._TRANSPORT) }
            if exists(._UID) { .systemd.t.UID = del(._UID) }

            # fields that are directly passed from clients and stored in the journal.
            .systemd.u = {}
            if exists(.CODE_FILE) { .systemd.u.CODE_FILE = del(.CODE_FILE) }
            if exists(.CODE_FUNC) { .systemd.u.CODE_FUNCTION = del(.CODE_FUNC) }
            if exists(.CODE_LINE) { .systemd.u.CODE_LINE = del(.CODE_LINE) }
            if exists(.ERRNO) { .systemd.u.ERRNO = del(.ERRNO) }
            if exists(.MESSAGE_ID) { .systemd.u.MESSAGE_ID = del(.MESSAGE_ID) }
            if exists(.SYSLOG_FACILITY) { .systemd.u.SYSLOG_FACILITY = del(.SYSLOG_FACILITY) }
            if exists(.SYSLOG_IDENTIFIER) { .systemd.u.SYSLOG_IDENTIFIER = del(.SYSLOG_IDENTIFIER) }
            if exists(.SYSLOG_PID) { .systemd.u.SYSLOG_PID = del(.SYSLOG_PID) }
            if exists(.RESULT) { .systemd.u.RESULT = del(.RESULT) }
            if exists(.UNIT) { .systemd.u.UNIT = del(.UNIT) }

            .time = format_timestamp!(.timestamp, format: "%FT%T%:z")

            ts = del(.timestamp); if !exists(."@timestamp") {."@timestamp" = ts}
      host_audit_logs:
        type: remap
        inputs:
          - raw_host_audit_logs
        source: >2
            .tag = ".linux-audit.log"

            match1 = parse_regex(.message, r'type=(?P<type>[^ ]+)') ?? {}
            envelop = {}
            envelop |= {"type": match1.type}

            match2, err = parse_regex(.message, r'msg=audit\((?P<ts_record>[^ ]+)\):')
            if err == null {
              sp = split(match2.ts_record,":") ?? ""
              if length(sp) == 2 {
                  ts = parse_timestamp(sp[0],"%s.%3f") ?? ""
                  envelop |= {"record_id": sp[1]}
                  . |= {"audit.linux" : envelop}
                  . |= {"@timestamp" : format_timestamp(ts,"%+") ?? ""}
              }
            } else {
              log("could not parse host audit msg. err=" + err, rate_limit_secs: 0)
            }

            .level = "default"
      k8s_audit_logs:
        type: remap
        inputs:
          - raw_k8s_audit_logs
        source: |2
            .tag = ".k8s-audit.log"
            . = merge(., parse_json!(string!(.message))) ?? .
            del(.message)
            .k8s_audit_level = .level
            .level = "default"
      openshift_audit_logs:
        type: remap
        inputs:
          - raw_openshift_audit_logs
        source: |2
            .tag = ".openshift-audit.log"
            . = merge(., parse_json!(string!(.message))) ?? .
            del(.message)
            .openshift_audit_level = .level
            .level = "default"
      ovn_audit_logs:
        type: remap
        inputs:
          - raw_ovn_audit_logs
        source: >2
            .tag = ".ovn-audit.log"
            if !exists(.level) {
              .level = "default"
              if match!(.message, r'Info|INFO|^I[0-9]+|level=info|Value:info|"level":"info"|<info>') {
                .level = "info"
              } else if match!(.message, r'Warning|WARN|^W[0-9]+|level=warn|Value:warn|"level":"warn"|<warn>') {
                .level = "warn"
              } else if match!(.message, r'Error|ERROR|^E[0-9]+|level=error|Value:error|"level":"error"|<error>') {
                .level = "error"
              } else if match!(.message, r'Critical|CRITICAL|^C[0-9]+|level=critical|Value:critical|"level":"critical"|<critical>') {
                .level = "critical"
              } else if match!(.message, r'Debug|DEBUG|^D[0-9]+|level=debug|Value:debug|"level":"debug"|<debug>') {
                .level = "debug"
              } else if match!(.message, r'Notice|NOTICE|^N[0-9]+|level=notice|Value:notice|"level":"notice"|<notice>') {
                .level = "notice"
              } else if match!(.message, r'Alert|ALERT|^A[0-9]+|level=alert|Value:alert|"level":"alert"|<alert>') {
                .level = "alert"
              } else if match!(.message, r'Emergency|EMERGENCY|^EM[0-9]+|level=emergency|Value:emergency|"level":"emergency"|<emergency>') {
                .level = "emergency"
              }
            }
      route_container_logs:
        type: route
        inputs:
          - container_logs
        route:
          app: '!((starts_with!(.kubernetes.namespace_name,"kube-")) ||
            (starts_with!(.kubernetes.namespace_name,"openshift-")) ||
            (.kubernetes.namespace_name == "default") || (.kubernetes.namespace_name
            == "openshift") || (.kubernetes.namespace_name == "kube"))'
          infra: '(starts_with!(.kubernetes.namespace_name,"kube-")) ||
            (starts_with!(.kubernetes.namespace_name,"openshift-")) ||
            (.kubernetes.namespace_name == "default") || (.kubernetes.namespace_name
            == "openshift") || (.kubernetes.namespace_name == "kube")'
      infrastructure:
        type: remap
        inputs:
          - route_container_logs.infra
          - journal_logs
        source: |2
            .log_type = "infrastructure"
      audit:
        type: remap
        inputs:
          - host_audit_logs
          - k8s_audit_logs
          - openshift_audit_logs
          - ovn_audit_logs
        source: |2
            .log_type = "audit"
            .hostname = get_env_var("VECTOR_SELF_NODE_NAME") ?? ""
            ts = del(.timestamp); if !exists(."@timestamp") {."@timestamp" = ts}
      application:
        type: remap
        inputs:
          - route_container_logs.app
          - journal_logs
        source: |2
            .log_type = "application"
      # drop high cardinality fields, because it flood the prometheus
      internal_metrics_remove_tags:
        type: remap
        inputs:
          - internal_metrics
        source: |2-
              del(.tags.file)
              del(.tags.pod_name)
              del(.tags.pod_namespace)
      # drop sudden high cardinality tags
      internal_metrics_cardinality:
        type: tag_cardinality_limit
        inputs:
          - internal_metrics_remove_tags
        mode: exact
        value_limit: 50
      # drop high cardinality metrics
      drop_transform_metrics:
        type: filter
        inputs:
          - internal_metrics_cardinality
        condition: |-
          (.name != "component_received_events_count" && .tags.component_kind != "transform") ||
          (.tags.component_kind != "transform" && .tags.component_type != "prometheus_exporter" && .tags.component_type != "internal_metrics")
      # add hostname to events
      add_nodename_to_metric:
        type: remap
        inputs:
          - drop_transform_metrics
        source: |
          .tags.hostname = get_env_var!("VECTOR_SELF_NODE_NAME")
    sinks:
      prometheus_output:
        type: prometheus_exporter
        inputs:
          - add_nodename_to_metric
        address: "[::]:9090"
        default_namespace: collector
      blackhole:
        print_interval_secs: 0
        type: blackhole
        inputs:
        - route_container_logs._unmatched
      platform-logs-infra:
        address: infra-aggregator-vector.{{ .Release.Namespace }}.svc.cluster.local:9000
        buffer:
          max_size: 502450000
          type: disk
          when_full: block
        healthcheck:
          enabled: false
        inputs:
          - infrastructure
          - audit
        type: vector
        version: "2"
      platform-logs-app:
        address: app-aggregator-vector.{{ .Release.Namespace }}.svc.cluster.local:9000
        buffer:
          max_size: 502450000
          type: disk
          when_full: block
        healthcheck:
          enabled: false
        inputs:
          - application
        type: vector
        version: "2"
